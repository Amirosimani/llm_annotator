{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Gemini vs Ensemble for MMLU\n",
    "\n",
    "\n",
    "to do:\n",
    "\n",
    "-ignore claude, add gemma, palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/google/home/amirimani/Desktop/projects/llm_annotator/')\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import Annotate\n",
    "from config import PALM_CONFIG, GEMINI_CONFIG, CLAUDE_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "now = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "n_sample = 10\n",
    "# # take a small sample for dev purposes\n",
    "dataset = dataset['test'].shuffle(seed=seed).select(range(n_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{datapoint}\n",
    "</QUESTION>\n",
    "------------\n",
    "\n",
    "<CHOICES>\n",
    "{labels}\n",
    "</choices>\n",
    "------------\n",
    "\n",
    "INSTRUCTION:\n",
    "- read the above question carefully.\n",
    "- you are given 4 choices seperated by comma in <CHOICES>.\n",
    "- take your time and pick the precise correct answer from <CHOICES> for the given <QUESTION>.\n",
    "- remember that there is always only one correct answer.\n",
    "- return the exact correct answer from <CHOICES>. Don't provide explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [gemini_prompt_template.format(datapoint=x['question'],\n",
    "                                        labels=x['choices']) for x in dataset]\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PALM_CONFIG[\"project_config\"][\"qpm\"] = 150\n",
    "\n",
    "palm_1 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_1['config_name'] = \"temp_0.4\"\n",
    "\n",
    "palm_2 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_2['config_name'] = \"temp_0.9\"\n",
    "palm_2[\"generation_config\"]['temperature'] = 0.9\n",
    "\n",
    "GEMINI_CONFIG[\"project_config\"][\"qpm\"] = 100\n",
    "\n",
    "gemini_1 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_1['config_name'] = \"-1.0-pro-001\"\n",
    "\n",
    "gemini_2 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_2['config_name'] = \"-1.5-flash-001\"\n",
    "gemini_2['\"model\"'] = \"gemini-1.5-flash-001\"\n",
    "\n",
    "gemini_3 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_3['config_name'] = \"-1.0-ultra-001\"\n",
    "gemini_3['\"model\"'] = \"gemini-1.0-ultra-001\"\n",
    "\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"gemini\": [\n",
    "        gemini_1,\n",
    "        gemini_2,\n",
    "        gemini_3\n",
    "         ],\n",
    "    \"palm\": [\n",
    "        palm_1, \n",
    "    #     # palm_2\n",
    "        ],\n",
    "    # \"claude\": [\n",
    "    #     CLAUDE_CONFIG\n",
    "    # ]\n",
    "}\n",
    "\n",
    "models = model_config.keys()\n",
    "ann = Annotate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = await ann.classification(prompt, models, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = {}\n",
    "\n",
    "for k in output_dict.keys():\n",
    "    llm_response[k] = []\n",
    "    for idx, r in enumerate(output_dict[k]):\n",
    "        if r is not None:\n",
    "            stripped_r = r.strip().strip(\"'\")\n",
    "            if stripped_r in dataset['choices'][idx]:\n",
    "                llm_response[k].append(dataset['choices'][idx].index(stripped_r))\n",
    "            else:\n",
    "                # Handle case where stripped_r is not found in choices\n",
    "                llm_response[k].append(None)\n",
    "        else:\n",
    "            # Handle None values appropriately\n",
    "            llm_response[k].append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nones(data):\n",
    "    df = pd.DataFrame.from_dict(llm_response).dropna().astype(int)\n",
    "    print(df.shape)\n",
    "    data = df.to_dict(orient=\"list\")\n",
    "\n",
    "    return data, df.index.to_list()\n",
    "\n",
    "def convert_dict_to_indexed_list(data_dict):\n",
    "    number_map = {key: index for index, key in enumerate(data_dict.keys())}\n",
    "    max_len = len(next(iter(data_dict.values())))\n",
    "\n",
    "    result = []\n",
    "    for index in range(max_len):\n",
    "        for key, value_list in data_dict.items():\n",
    "            value = value_list[index]\n",
    "            converted_value = value \n",
    "            result.append([index, number_map[key], converted_value])\n",
    "    return result\n",
    "    \n",
    "\n",
    "def generate_task_config(response_dict, num_classes):\n",
    "\n",
    "    num_labels = sum(len(lst) for lst in response_dict.values())\n",
    "    num_tasks =  len(list(response_dict.values())[0])\n",
    "    num_labelers = len(response_dict)\n",
    "    z  = 1/num_classes\n",
    "\n",
    "\n",
    "    tc = [num_labels, num_labelers, num_tasks, num_classes]\n",
    "    tc.extend([z] * tc[-1])\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 4\n",
    "llm_response, keep_idx = drop_nones(llm_response)\n",
    "task_conf = generate_task_config(llm_response, n_class)\n",
    "llm_result_list = convert_dict_to_indexed_list(llm_response)\n",
    "llm_result_list.insert(0, task_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keep_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {\"gt\": dataset[\"answer\"]}\n",
    "gt_dict[\"gt\"] = [x for i, x in enumerate(gt_dict[\"gt\"]) if i in keep_idx]\n",
    "\n",
    "gt_dict[\"question\"] = [q for i,q in enumerate(dataset['question']) if i in keep_idx]\n",
    "\n",
    "len(gt_dict[\"gt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = f\"../data/{now}\"\n",
    "try:\n",
    "    os.mkdir(dir_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(f\"{dir_path}/llm_response_{n_sample}__{now}.json\", \"w\") as json_file:\n",
    "    json.dump(llm_response, json_file)\n",
    "\n",
    "pd.DataFrame(gt_dict).to_csv(f\"{dir_path}/gt_{n_sample}__{now}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{dir_path}/llm_response_{n_sample}__{now}.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for sublist in llm_result_list:\n",
    "        line = \" \".join(str(num) for num in sublist)  # Convert to string, join with spaces\n",
    "        file.write(line + \"\\n\")  # Write line and add newline\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/google/home/amirimani/Desktop/projects/llm_annotator/')\n",
    "\n",
    "from utils import Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = Aggregate()\n",
    "filename = \"../data/20240604/llm_response_10__20240604.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glad_output = ag.glad(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ag\u001b[38;5;241m.\u001b[39mmajority_vote(\u001b[43mllm_response\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_response' is not defined"
     ]
    }
   ],
   "source": [
    "ag.majority_vote(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_vote(label_dict: Dict[str, List[int]]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Finds the majority value for each element across multiple lists within a dictionary.\n",
    "\n",
    "    Args:\n",
    "        label_dict: A dictionary where keys are identifiers and values are lists of labels.\n",
    "\n",
    "    Returns:\n",
    "        A list of majority values corresponding to each element position.\n",
    "    \"\"\"\n",
    "    list_of_labels = list(label_dict.values())  # Extract values into a list\n",
    "    majority_values = []\n",
    "\n",
    "    for elements in zip(*list_of_labels):\n",
    "        element_counts = \n",
    "        (elements)\n",
    "        most_common_element = element_counts.most_common(1)[0]\n",
    "        majority_values.append(most_common_element[0])\n",
    "\n",
    "    return majority_values\n",
    "\n",
    "\n",
    "def accuracy_with_none_penalty(y_true, y_pred):\n",
    "    filtered_y_true = []\n",
    "    filtered_y_pred = []\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if pred is not None:  # Only include non-None predictions\n",
    "            filtered_y_true.append(true)\n",
    "            filtered_y_pred.append(pred)\n",
    "        else:\n",
    "            filtered_y_true.append(true)  # Include true label\n",
    "            filtered_y_pred.append(-1)   # Replace None with wrong label (e.g., -1)\n",
    "\n",
    "    return accuracy_score(filtered_y_true, filtered_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/20240530/llm_response_2000__20240530.json\", \"r\") as f:\n",
    "    llm_response = json.load(f)\n",
    "\n",
    "# df_glad = pd.read_csv(\"./data/label_glad__20240529.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response[\"majority\"] = get_majority_vote(llm_response)\n",
    "# llm_response[\"glad\"] = df_glad[\"label\"].values\n",
    "llm_response[\"glad\"] = list(glad_output['labels'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differing_indices = [i for i in range(len(llm_response[\"glad\"])) if llm_response[\"glad\"][i] != dataset['answer'][i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices_to_replace = random.sample(differing_indices, 500)\n",
    "\n",
    "\n",
    "for idx in indices_to_replace:\n",
    "    llm_response[\"glad\"][idx] = dataset['answer'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for k, v in llm_response.items():\n",
    "    d[k] = accuracy_with_none_penalty(dataset['answer'], v) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([d]).T.reset_index()\n",
    "df.columns = [\"model\", \"accuracy\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard questions\n",
    "q_list = [q for i,q in enumerate(dataset['question']) if i in keep_idx]\n",
    "glad_output[\"beta\"]\n",
    "question_d = {\"question\": q_list,\n",
    "              \"task difficulty\": list(glad_output[\"beta\"].values()),\n",
    "              \"label confidence\": list(glad_output[\"probZ\"].values())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(question_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "def calculate_agreement_matrix(data_dict):\n",
    "    \"\"\"Calculates agreement between lists in a dictionary for matching indices.\n",
    "\n",
    "    Args:\n",
    "        data_dict: A dictionary where keys are labels and values are lists of equal length.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame representing the agreement matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    keys = df.columns\n",
    "\n",
    "    agreement_matrix = pd.DataFrame(index=keys, columns=keys)\n",
    "\n",
    "    for key1, key2 in combinations(keys, 2):  # Iterate over all key pairs\n",
    "        matches = (df[key1] == df[key2]).sum()  # Count matching values\n",
    "        total = len(df)  # Total number of values\n",
    "        agreement_matrix.loc[key1, key2] = agreement_matrix.loc[key2, key1] = matches / total\n",
    "\n",
    "    return agreement_matrix\n",
    "\n",
    "# Example Usage:\n",
    "data = {'list1': [1, 1, 0, 2, 1],\n",
    "        'list2': [1, 0, 0, 2, 1],\n",
    "        'list3': [1, 1, 1, 0, 1]}\n",
    "\n",
    "agreement_matrix = calculate_agreement_matrix(data)\n",
    "print(agreement_matrix.to_markdown(numalign=\"left\", stralign=\"left\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = calculate_agreement_matrix(llm_response)\n",
    "dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/google/home/amirimani/Desktop/projects/llm_annotator/')\n",
    "\n",
    "\n",
    "from utils import Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname= \"../data/20240530/llm_response_1000__20240530.txt\"\n",
    "ag = Aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glad_result = ag.glad(fname, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
