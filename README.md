# LLM Annotators

Large Language Models (LLMs) have opened up exciting possibilities for AI applications that were previously difficult or unattainable. However, evaluating their responses in a systematic way presents real challenges. This project proposes a solution: an ensemble approach where a panel of LLMs act as judges, voting on the accuracy of answers generated by another LLM.

* Available tasks: Classfication (majority vote, GLAD <sup>1</sup> )
# How to use this?

You can follow the notebook under `/notebook/ghack.ipynb` for a sample implementation. 


## Run the Streamlit app locally

The streamlit app is for demo purpose only.

- create a virutal env `python3 -m venv .venv`
- activate the env `source .venv/bin/activate`
- install requirments `pip install -r requirments.txt`
- **local secret management**: `.streamlit/secrets.toml` for local runs. make sure you add it to `.gitignore`
- run the app:
 - `streamlit run ./app/üè†_Home.py`




# References
1. [GLAD paper](https://proceedings.neurips.cc/paper_files/paper/2009/file/f899139df5e1059396431415e770c6dd-Paper.pdf) and implementation [referece](https://github.com/notani/python-glad/blob/master/glad.py#L58)
