{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Gemini vs Ensemble for MMLU\n",
    "\n",
    "\n",
    "to do:\n",
    "\n",
    "-ignore claude, add gemma, palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import Annotate\n",
    "from config import PALM_CONFIG, GEMINI_CONFIG, CLAUDE_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "now = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "\n",
    "# # take a small sample for dev purposes\n",
    "dataset = dataset['test'].shuffle(seed=seed).select(range(100))\n",
    "\n",
    "# # user provided data description\n",
    "# DESCRIPTION = \"\"\"\n",
    "# This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge.\n",
    "# The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn.\n",
    "# To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.\n",
    "# This covers 57 subjects  across STEM, the humanities, the social sciences, and more. \n",
    "# It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. \n",
    "# Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics.\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{datapoint}\n",
    "</QUESTION>\n",
    "------------\n",
    "\n",
    "<CHOICES>\n",
    "{labels}\n",
    "</choices>\n",
    "------------\n",
    "\n",
    "INSTRUCTION:\n",
    "- read the above question carefully.\n",
    "- you are given 4 choices seperated by comma in <CHOICES>.\n",
    "- take your time and pick the precise correct answer from <CHOICES> for the given <QUESTION>.\n",
    "- remember that there is always only one correct answer.\n",
    "- return the exact correct answer from <CHOICES>. Don't provide explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [gemini_prompt_template.format(datapoint=x['question'],\n",
    "                                        labels=x['choices']) for x in dataset]\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"palm\",\n",
    "    \"gemini\",\n",
    "    # \"claude\"\n",
    "    ]\n",
    "\n",
    "PALM_CONFIG[\"project_config\"][\"qpm\"] = 150\n",
    "\n",
    "palm_1 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_1['config_name'] = \"temp_0.4\"\n",
    "\n",
    "palm_2 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_2['config_name'] = \"temp_0.9\"\n",
    "palm_2[\"generation_config\"]['temperature'] = 0.9\n",
    "\n",
    "GEMINI_CONFIG[\"project_config\"][\"qpm\"] = 100\n",
    "\n",
    "gemini_1 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_1['config_name'] = \"-1.0-pro-002\"\n",
    "\n",
    "gemini_2 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_2['config_name'] = \"-1.5-flash-001\"\n",
    "gemini_2['\"model\"'] = \"gemini-1.5-flash-001\"\n",
    "\n",
    "gemini_3 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_3['config_name'] = \"-1.0-ultra-001\"\n",
    "gemini_3['\"model\"'] = \"gemini-1.0-ultra-001\"\n",
    "\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"gemini\": [\n",
    "        gemini_1,\n",
    "        gemini_2,\n",
    "        gemini_3\n",
    "         ],\n",
    "    \"palm\": [\n",
    "        palm_1, \n",
    "        palm_2\n",
    "        ],\n",
    "    # \"claude\": [\n",
    "    #     CLAUDE_CONFIG\n",
    "    # ]\n",
    "}\n",
    "\n",
    "ann = Annotate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = await ann.classification(prompt, models, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = {}\n",
    "\n",
    "for k in output_dict.keys():\n",
    "    llm_response[k] = []\n",
    "    for idx, r in enumerate(output_dict[k]):\n",
    "        if r is not None:\n",
    "            stripped_r = r.strip().strip(\"'\")\n",
    "            if stripped_r in dataset['choices'][idx]:\n",
    "                llm_response[k].append(dataset['choices'][idx].index(stripped_r))\n",
    "            else:\n",
    "                # Handle case where stripped_r is not found in choices\n",
    "                llm_response[k].append(None)\n",
    "        else:\n",
    "            # Handle None values appropriately\n",
    "            llm_response[k].append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/raw_llm_response__{now}.json\", \"w\") as json_file:\n",
    "    json.dump(llm_response, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_none_with_random(data, n):\n",
    "    new_data = {}  \n",
    "    for key, lst in data.items():\n",
    "        new_lst = []  \n",
    "        for value in lst:\n",
    "            if value is None:\n",
    "                new_lst.append(random.randint(0, n))\n",
    "            else:\n",
    "                new_lst.append(value)\n",
    "        new_data[key] = new_lst \n",
    "    return new_data\n",
    "\n",
    "\n",
    "\n",
    "def convert_dict_to_indexed_list(data_dict):\n",
    "    number_map = {key: index for index, key in enumerate(data_dict.keys())}\n",
    "    max_len = len(next(iter(data_dict.values())))\n",
    "\n",
    "    result = []\n",
    "    for index in range(max_len):\n",
    "        for key, value_list in data_dict.items():\n",
    "            value = value_list[index]\n",
    "            # Convert to 0 if not an integer\n",
    "            converted_value = 0 if not isinstance(value, int) else value \n",
    "            result.append([index, number_map[key], converted_value])\n",
    "    return result\n",
    "    \n",
    "\n",
    "def generate_task_config(response_dict, num_classes):\n",
    "\n",
    "    num_labels = sum(len(lst) for lst in response_dict.values())\n",
    "    num_tasks =  len(list(response_dict.values())[0])\n",
    "    num_labelers = len(response_dict)\n",
    "    z  = 1/num_classes\n",
    "\n",
    "\n",
    "    tc = [num_labels, num_labelers, num_tasks, num_classes]\n",
    "    tc.extend([z] * tc[-1])\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/raw_llm_response__{now}.json\", \"w\") as json_file:\n",
    "    json.dump(llm_response, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 4\n",
    "llm_response_filled = replace_none_with_random(llm_response, n_class)\n",
    "\n",
    "\n",
    "task_conf = generate_task_config(llm_response_filled, n_class)\n",
    "llm_result_list = convert_dict_to_indexed_list(llm_response_filled)\n",
    "llm_result_list.insert(0, task_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"./data/llm_response_100__{now}.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for sublist in llm_result_list:\n",
    "        line = \" \".join(str(num) for num in sublist)  # Convert to string, join with spaces\n",
    "        file.write(line + \"\\n\")  # Write line and add newline\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import glad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glad_output = glad(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_vote(label_dict: Dict[str, List[int]]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Finds the majority value for each element across multiple lists within a dictionary.\n",
    "\n",
    "    Args:\n",
    "        label_dict: A dictionary where keys are identifiers and values are lists of labels.\n",
    "\n",
    "    Returns:\n",
    "        A list of majority values corresponding to each element position.\n",
    "    \"\"\"\n",
    "    list_of_labels = list(label_dict.values())  # Extract values into a list\n",
    "    majority_values = []\n",
    "\n",
    "    for elements in zip(*list_of_labels):\n",
    "        element_counts = Counter(elements)\n",
    "        most_common_element = element_counts.most_common(1)[0]\n",
    "        majority_values.append(most_common_element[0])\n",
    "\n",
    "    return majority_values\n",
    "\n",
    "\n",
    "def accuracy_with_none_penalty(y_true, y_pred):\n",
    "    filtered_y_true = []\n",
    "    filtered_y_pred = []\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if pred is not None:  # Only include non-None predictions\n",
    "            filtered_y_true.append(true)\n",
    "            filtered_y_pred.append(pred)\n",
    "        else:\n",
    "            filtered_y_true.append(true)  # Include true label\n",
    "            filtered_y_pred.append(-1)   # Replace None with wrong label (e.g., -1)\n",
    "\n",
    "    return accuracy_score(filtered_y_true, filtered_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 3, 2, 1, 1, 2, 2, 2, 1, 2, 0, 2, 1, 1, 0, 1, 2, 1, 3, 2,\n",
       "       1, 2, 2, 1, 1, 1, 2, 1, 3, 2, 0, 1, 1, 1, 0, 2, 2, 0, 3, 2, 1, 2,\n",
       "       3, 3, 1, 1, 3, 3, 2, 2, 2, 3, 1, 1, 0, 2, 3, 2, 3, 3, 1, 2, 2, 3,\n",
       "       3, 2, 3, 1, 1, 1, 3, 2, 1, 3, 1, 2, 1, 1, 2, 2, 0, 1, 0, 2, 2, 0,\n",
       "       0, 1, 2, 0, 1, 0, 2, 0, 1, 2, 1, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/raw_llm_response_100__20240529.json\", \"r\") as f:\n",
    "    llm_response = json.load(f)\n",
    "\n",
    "df_glad = pd.read_csv(\"./data/label_glad__20240529.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response[\"majority\"] = get_majority_vote(llm_response)\n",
    "llm_response[\"glad\"] = df_glad[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palm_temp_0.4 0.58\n",
      "palm_temp_0.9 0.58\n",
      "gemini_-1.0-pro-002 0.55\n",
      "gemini_-1.5-flash-001 0.55\n",
      "gemini_-1.0-ultra-001 0.53\n",
      "majority 0.52\n",
      "glad 0.6\n"
     ]
    }
   ],
   "source": [
    "for k, v in llm_response.items():\n",
    "    print(k, accuracy_with_none_penalty(dataset['answer'], v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "return {\"accuracy\": accuracy_score(y_true, y_pred),\n",
    "                \"f1_weighted\": f1_score(y_true, y_pred, average='weighted'),\n",
    "                \"confusion_matrix\": confusion_matrix(y_true, y_pred)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
