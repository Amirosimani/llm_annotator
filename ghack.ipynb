{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Gemini vs Ensemble for MMLU\n",
    "\n",
    "\n",
    "to do:\n",
    "\n",
    "-ignore claude, add gemma, palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import Annotate\n",
    "from config import PALM_CONFIG, GEMINI_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "now = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "\n",
    "# # take a small sample for dev purposes\n",
    "dataset = dataset['test'].shuffle(seed=seed).select(range(5))\n",
    "\n",
    "# user provided data description\n",
    "DESCRIPTION = \"\"\"\n",
    "This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge.\n",
    "The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn.\n",
    "To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.\n",
    "This covers 57 subjects  across STEM, the humanities, the social sciences, and more. \n",
    "It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. \n",
    "Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{datapoint}\n",
    "</QUESTION>\n",
    "------------\n",
    "\n",
    "<CHOICES>\n",
    "{labels}\n",
    "</choices>\n",
    "------------\n",
    "\n",
    "INSTRUCTION:\n",
    "- read the above question carefully.\n",
    "- you are given 4 choices seperated by comma in <CHOICES>.\n",
    "- take your time and pick the precise correct answer from <CHOICES> for the given <QUESTION>.\n",
    "- remember that there is always only one correct answer.\n",
    "- return the exact correct answer from <CHOICES>. Don't provide explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "prompt = [gemini_prompt_template.format(description= DESCRIPTION,\n",
    "                                        datapoint=x['question'],\n",
    "                                        labels=x['choices']) for x in dataset]\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"palm\",\n",
    "    \"gemini\",\n",
    "    # \"claude\"\n",
    "    ]\n",
    "\n",
    "\n",
    "palm_1 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_1['config_name'] = \"palm_1\"\n",
    "palm_2 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_2['config_name'] = \"palm_2\"\n",
    "\n",
    "gemini_1 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_1['config_name'] = \"gemini_1\"\n",
    "gemini_2 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_2['config_name'] = \"gemini_2\"\n",
    "\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"gemini\": [gemini_1, gemini_2],\n",
    "    \"palm\": [palm_1, palm_2]\n",
    "    # \"palm\": PALM_CONFIG\n",
    "}\n",
    "\n",
    "ann = Annotate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating tasks: 100%|██████████| 20/20 [00:00<00:00, 65179.55it/s]\n",
      "Gathering palm_palm_1 results: 100%|██████████| 5/5 [00:13<00:00,  2.60s/it]\n",
      "Gathering palm_palm_2 results: 100%|██████████| 5/5 [00:00<00:00, 26990.37it/s]\n",
      "Gathering gemini_gemini_1 results:   0%|          | 0/5 [00:00<?, ?it/s]2024-05-29 18:54:09,407/Annotate[ERROR]: gemini_gemini_1 Task 0 failed: Cannot get the response text.\n",
      "Cannot get the Candidate text.\n",
      "Response candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\n",
      "Content:\n",
      "{}\n",
      "Candidate:\n",
      "{\n",
      "  \"finish_reason\": \"OTHER\"\n",
      "}\n",
      "Response:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"finish_reason\": \"OTHER\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage_metadata\": {\n",
      "    \"prompt_token_count\": 168,\n",
      "    \"total_token_count\": 168\n",
      "  }\n",
      "}\n",
      "Gathering gemini_gemini_1 results: 100%|██████████| 5/5 [00:00<00:00, 3913.33it/s]\n",
      "Gathering gemini_gemini_2 results:   0%|          | 0/5 [00:00<?, ?it/s]2024-05-29 18:54:09,410/Annotate[ERROR]: gemini_gemini_2 Task 0 failed: Cannot get the response text.\n",
      "Cannot get the Candidate text.\n",
      "Response candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\n",
      "Content:\n",
      "{}\n",
      "Candidate:\n",
      "{\n",
      "  \"finish_reason\": \"OTHER\"\n",
      "}\n",
      "Response:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"finish_reason\": \"OTHER\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage_metadata\": {\n",
      "    \"prompt_token_count\": 168,\n",
      "    \"total_token_count\": 168\n",
      "  }\n",
      "}\n",
      "Gathering gemini_gemini_2 results: 100%|██████████| 5/5 [00:00<00:00, 5091.41it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dict = await ann.classification(prompt, models, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'palm_palm_1': [' Homo erectus.',\n",
       "  ' simple',\n",
       "  ' Perform preliminary analytical procedures to identify accounts that may represent specific risks relevant to the engagement.',\n",
       "  \" 'All of these options.'\",\n",
       "  ' Deep pyro sequencing (NGS)'],\n",
       " 'palm_palm_2': [' Homo erectus.',\n",
       "  ' simple',\n",
       "  ' Perform preliminary analytical procedures to identify accounts that may represent specific risks relevant to the engagement.',\n",
       "  \" 'All of these options.'\",\n",
       "  ' Deep pyro sequencing (NGS)'],\n",
       " 'gemini_gemini_1': [None,\n",
       "  \"'compound'\",\n",
       "  '\"Make inquiries of management concerning the entity\\'s procedures used in adjusting and closing the books of account.\"',\n",
       "  'All of these options.',\n",
       "  \"'Deep pyro sequencing (NGS)'\"],\n",
       " 'gemini_gemini_2': [None,\n",
       "  \"'compound'\",\n",
       "  '\"Make inquiries of management concerning the entity\\'s procedures used in adjusting and closing the books of account.\"',\n",
       "  '\"All of these options.\"',\n",
       "  'Deep pyro sequencing (NGS)']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'palm_palm_1': [2, 0, 1, 3, 2],\n",
       " 'palm_palm_2': [2, 0, 1, 3, 2],\n",
       " 'gemini_gemini_1': [None, 1, None, 3, 2],\n",
       " 'gemini_gemini_2': [None, 1, None, None, 2]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response = {}\n",
    "\n",
    "for k in output_dict.keys():\n",
    "    llm_response[k] = []\n",
    "    for idx, r in enumerate(output_dict[k]):\n",
    "        if r is not None:\n",
    "            stripped_r = r.strip().strip(\"'\")\n",
    "            if stripped_r in dataset['choices'][idx]:\n",
    "                llm_response[k].append(dataset['choices'][idx].index(stripped_r))\n",
    "            else:\n",
    "                # Handle case where stripped_r is not found in choices\n",
    "                llm_response[k].append(None)\n",
    "        else:\n",
    "            # Handle None values appropriately\n",
    "            llm_response[k].append(None)\n",
    "\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import glad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 5, 4, 4, 0.25, 0.25, 0.25, 0.25]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = sum(len(lst) for lst in llm_response.values())\n",
    "num_labelers =  len(list(llm_response.values())[0])\n",
    "num_tasks = len(llm_response)\n",
    "num_classes = 4\n",
    "z  = 1/num_classes\n",
    "\n",
    "\n",
    "tc = [num_labels, num_labelers, num_tasks, num_classes]\n",
    "tc.extend([z] * tc[-1])\n",
    "\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('./data/output/annotation_output__20240515.json', 'r') as file:\n",
    "#     ddddd = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mglad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/llm_annotator/utils.py:535\u001b[0m, in \u001b[0;36mglad\u001b[0;34m(data, task_config)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mglad\u001b[39m(data, task_config):\n\u001b[0;32m--> 535\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m     EM(data)\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output(data)\n",
      "File \u001b[0;32m~/Desktop/projects/llm_annotator/utils.py:254\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(sample_data, task_config)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[1;32m    253\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead: task(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m by labeler \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(task, label, labeler))\n\u001b[0;32m--> 254\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m[labeler] \u001b[38;5;241m=\u001b[39m label \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Initialize Probs\u001b[39;00m\n\u001b[1;32m    256\u001b[0m data\u001b[38;5;241m.\u001b[39mpriorAlpha \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(data\u001b[38;5;241m.\u001b[39mnumLabelers)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "glad(llm_response, tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
