{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Gemini vs Ensemble for MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import Annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed =42\n",
    "now = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "\n",
    "# # take a small sample for dev purposes\n",
    "dataset = dataset['test'].shuffle(seed=seed).select(range(20))\n",
    "\n",
    "# user provided data description\n",
    "DESCRIPTION = \"\"\"\n",
    "This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge.\n",
    "The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn.\n",
    "To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.\n",
    "This covers 57 subjects  across STEM, the humanities, the social sciences, and more. \n",
    "It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. \n",
    "Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{datapoint}\n",
    "</QUESTION>\n",
    "------------\n",
    "\n",
    "<CHOICES>\n",
    "{labels}\n",
    "</choices>\n",
    "------------\n",
    "\n",
    "INSTRUCTION:\n",
    "- read the above question carefully.\n",
    "- you are given 4 choices seperated by comma in <CHOICES>.\n",
    "- take your time and pick the precise correct answer from <CHOICES> for the given <QUESTION>.\n",
    "- remember that there is always only one correct answer.\n",
    "- return the exact correct answer from <CHOICES>. Don't provide explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "prompt = [gemini_prompt_template.format(description= DESCRIPTION,\n",
    "                                        datapoint=x['question'],\n",
    "                                        labels=x['choices']) for x in dataset]\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # \"palm\",\n",
    "    # \"gemini\",\n",
    "    \"claude\"\n",
    "    ]\n",
    "\n",
    "ann = Annotate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating tasks: 40it [00:00, 153076.79it/s]           \n",
      "Gathering claude results:   0%|          | 0/20 [00:00<?, ?it/s]2024-05-28 22:23:54,813/Annotate[ERROR]: claude Task 0 failed: 'Message' object has no attribute 'text'\n",
      "Gathering claude results:   5%|▌         | 1/20 [00:26<08:22, 26.45s/it]2024-05-28 22:23:54,815/Annotate[ERROR]: claude Task 1 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,816/Annotate[ERROR]: claude Task 2 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,817/Annotate[ERROR]: claude Task 3 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,818/Annotate[ERROR]: claude Task 4 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,818/Annotate[ERROR]: claude Task 5 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,819/Annotate[ERROR]: claude Task 6 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,820/Annotate[ERROR]: claude Task 7 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,820/Annotate[ERROR]: claude Task 8 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,821/Annotate[ERROR]: claude Task 9 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,822/Annotate[ERROR]: claude Task 10 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,823/Annotate[ERROR]: claude Task 11 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,823/Annotate[ERROR]: claude Task 12 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,824/Annotate[ERROR]: claude Task 13 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,825/Annotate[ERROR]: claude Task 14 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,826/Annotate[ERROR]: claude Task 15 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,827/Annotate[ERROR]: claude Task 16 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,827/Annotate[ERROR]: claude Task 17 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,828/Annotate[ERROR]: claude Task 18 failed: 'Message' object has no attribute 'text'\n",
      "2024-05-28 22:23:54,829/Annotate[ERROR]: claude Task 19 failed: 'Message' object has no attribute 'text'\n",
      "Gathering claude results: 100%|██████████| 20/20 [00:26<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "r = await ann.classification(prompt, models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['claude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [dataset['choices'][idx][v] for idx, v in enumerate(dataset['answer'])]\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
