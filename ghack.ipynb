{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Gemini vs Ensemble for MMLU\n",
    "\n",
    "\n",
    "to do:\n",
    "\n",
    "-ignore claude, add gemma, palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import Annotate\n",
    "from config import PALM_CONFIG, GEMINI_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "now = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "\n",
    "# # take a small sample for dev purposes\n",
    "dataset = dataset['test'].shuffle(seed=seed).select(range(5))\n",
    "\n",
    "# user provided data description\n",
    "DESCRIPTION = \"\"\"\n",
    "This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge.\n",
    "The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn.\n",
    "To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.\n",
    "This covers 57 subjects  across STEM, the humanities, the social sciences, and more. \n",
    "It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. \n",
    "Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{datapoint}\n",
    "</QUESTION>\n",
    "------------\n",
    "\n",
    "<CHOICES>\n",
    "{labels}\n",
    "</choices>\n",
    "------------\n",
    "\n",
    "INSTRUCTION:\n",
    "- read the above question carefully.\n",
    "- you are given 4 choices seperated by comma in <CHOICES>.\n",
    "- take your time and pick the precise correct answer from <CHOICES> for the given <QUESTION>.\n",
    "- remember that there is always only one correct answer.\n",
    "- return the exact correct answer from <CHOICES>. Don't provide explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [gemini_prompt_template.format(description= DESCRIPTION,\n",
    "                                        datapoint=x['question'],\n",
    "                                        labels=x['choices']) for x in dataset]\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"palm\",\n",
    "    \"gemini\",\n",
    "    # \"claude\"\n",
    "    ]\n",
    "\n",
    "\n",
    "palm_1 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_1['config_name'] = \"palm_1\"\n",
    "palm_2 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_2['config_name'] = \"palm_2\"\n",
    "palm_2[\"generation_config\"]['temperature'] = 0.9\n",
    "\n",
    "\n",
    "gemini_1 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_1['config_name'] = \"gemini_1\"\n",
    "gemini_2 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_2['config_name'] = \"gemini_2\"\n",
    "gemini_2['\"model\"'] = \"gemini-1.5-flash-001\"\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"gemini\": [\n",
    "        gemini_1,\n",
    "        gemini_2\n",
    "         ],\n",
    "    \"palm\": [\n",
    "        palm_1, \n",
    "        palm_2\n",
    "        ]\n",
    "}\n",
    "\n",
    "ann = Annotate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = await ann.classification(prompt, models, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = {}\n",
    "\n",
    "for k in output_dict.keys():\n",
    "    llm_response[k] = []\n",
    "    for idx, r in enumerate(output_dict[k]):\n",
    "        if r is not None:\n",
    "            stripped_r = r.strip().strip(\"'\")\n",
    "            if stripped_r in dataset['choices'][idx]:\n",
    "                llm_response[k].append(dataset['choices'][idx].index(stripped_r))\n",
    "            else:\n",
    "                # Handle case where stripped_r is not found in choices\n",
    "                llm_response[k].append(None)\n",
    "        else:\n",
    "            # Handle None values appropriately\n",
    "            llm_response[k].append(None)\n",
    "\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import glad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task_config(response_dict, num_classes):\n",
    "    num_labels = sum(len(lst) for lst in response_dict.values())\n",
    "    num_labelers =  len(list(response_dict.values())[0])\n",
    "    num_tasks = len(response_dict)\n",
    "    z  = 1/num_classes\n",
    "\n",
    "\n",
    "    tc = [num_labels, num_labelers, num_tasks, num_classes]\n",
    "    tc.extend([z] * tc[-1])\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_conf = generate_task_config(llm_response, 4)\n",
    "task_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('./data/output/annotation_output__20240515.json', 'r') as file:\n",
    "#     sample_data = json.load(file)\n",
    "\n",
    "\n",
    "# task_conf = generate_task_config(sample_data, 2)\n",
    "# sample_data, task_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glad(llm_response, task_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
