{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Gemini vs Ensemble for MMLU\n",
    "\n",
    "\n",
    "to do:\n",
    "\n",
    "-ignore claude, add gemma, palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import Annotate\n",
    "from config import PALM_CONFIG, GEMINI_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "now = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "\n",
    "# # take a small sample for dev purposes\n",
    "dataset = dataset['test'].shuffle(seed=seed).select(range(5))\n",
    "\n",
    "# user provided data description\n",
    "DESCRIPTION = \"\"\"\n",
    "This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge.\n",
    "The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn.\n",
    "To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.\n",
    "This covers 57 subjects  across STEM, the humanities, the social sciences, and more. \n",
    "It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. \n",
    "Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{datapoint}\n",
    "</QUESTION>\n",
    "------------\n",
    "\n",
    "<CHOICES>\n",
    "{labels}\n",
    "</choices>\n",
    "------------\n",
    "\n",
    "INSTRUCTION:\n",
    "- read the above question carefully.\n",
    "- you are given 4 choices seperated by comma in <CHOICES>.\n",
    "- take your time and pick the precise correct answer from <CHOICES> for the given <QUESTION>.\n",
    "- remember that there is always only one correct answer.\n",
    "- return the exact correct answer from <CHOICES>. Don't provide explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "prompt = [gemini_prompt_template.format(description= DESCRIPTION,\n",
    "                                        datapoint=x['question'],\n",
    "                                        labels=x['choices']) for x in dataset]\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"palm\",\n",
    "    \"gemini\",\n",
    "    # \"claude\"\n",
    "    ]\n",
    "\n",
    "\n",
    "palm_1 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_1['name'] = \"palm_1\"\n",
    "palm_2 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_2['name'] = \"palm_2\"\n",
    "\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"gemini\": GEMINI_CONFIG,\n",
    "    # \"palm\": [palm_1, palm_2]\n",
    "    \"palm\": PALM_CONFIG\n",
    "}\n",
    "\n",
    "ann = Annotate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating tasks: 20it [00:00, 30404.52it/s]            \n",
      "Gathering palm results: 100%|██████████| 5/5 [00:06<00:00,  1.23s/it]\n",
      "Gathering gemini results:   0%|          | 0/5 [00:00<?, ?it/s]2024-05-29 02:28:24,393/Annotate[ERROR]: gemini Task 0 failed: Cannot get the response text.\n",
      "Cannot get the Candidate text.\n",
      "Response candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\n",
      "Content:\n",
      "{}\n",
      "Candidate:\n",
      "{\n",
      "  \"finish_reason\": \"OTHER\"\n",
      "}\n",
      "Response:\n",
      "{\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"finish_reason\": \"OTHER\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage_metadata\": {\n",
      "    \"prompt_token_count\": 217,\n",
      "    \"total_token_count\": 217\n",
      "  }\n",
      "}\n",
      "Gathering gemini results: 100%|██████████| 5/5 [00:00<00:00, 2210.32it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dict = await ann.classification(prompt, models, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = {}\n",
    "\n",
    "for m in models:   \n",
    "    llm_response[m] = [dataset['choices'][idx].index(r.strip().strip(\"'\")) for idx, r in enumerate(output_dict[m])]\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import glad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = sum(len(lst) for lst in llm_response.values())\n",
    "num_labelers =  len(list(llm_response.values())[0])\n",
    "num_tasks = len(llm_response)\n",
    "num_classes = 4\n",
    "z  = 1/num_classes\n",
    "\n",
    "\n",
    "tc = [num_labels, num_labelers, num_tasks, num_classes]\n",
    "tc.extend([z] * tc[-1])\n",
    "\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('./data/output/annotation_output__20240515.json', 'r') as file:\n",
    "#     ddddd = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glad(llm_response, tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
