{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Gemini vs Ensemble for MMLU\n",
    "\n",
    "\n",
    "to do:\n",
    "\n",
    "-ignore claude, add gemma, palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import Annotate\n",
    "from config import PALM_CONFIG, GEMINI_CONFIG, CLAUDE_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "now = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "n_sample = 50\n",
    "# # take a small sample for dev purposes\n",
    "dataset = dataset['test'].shuffle(seed=seed).select(range(n_sample))\n",
    "\n",
    "# # user provided data description\n",
    "# DESCRIPTION = \"\"\"\n",
    "# This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge.\n",
    "# The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn.\n",
    "# To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.\n",
    "# This covers 57 subjects  across STEM, the humanities, the social sciences, and more. \n",
    "# It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. \n",
    "# Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics.\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{datapoint}\n",
    "</QUESTION>\n",
    "------------\n",
    "\n",
    "<CHOICES>\n",
    "{labels}\n",
    "</choices>\n",
    "------------\n",
    "\n",
    "INSTRUCTION:\n",
    "- read the above question carefully.\n",
    "- you are given 4 choices seperated by comma in <CHOICES>.\n",
    "- take your time and pick the precise correct answer from <CHOICES> for the given <QUESTION>.\n",
    "- remember that there is always only one correct answer.\n",
    "- return the exact correct answer from <CHOICES>. Don't provide explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "prompt = [gemini_prompt_template.format(datapoint=x['question'],\n",
    "                                        labels=x['choices']) for x in dataset]\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"gemini\",\n",
    "    \"palm\",\n",
    "    # \"claude\"\n",
    "    ]\n",
    "\n",
    "PALM_CONFIG[\"project_config\"][\"qpm\"] = 150\n",
    "\n",
    "palm_1 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_1['config_name'] = \"temp_0.4\"\n",
    "\n",
    "palm_2 =  copy.deepcopy(PALM_CONFIG)\n",
    "palm_2['config_name'] = \"temp_0.9\"\n",
    "palm_2[\"generation_config\"]['temperature'] = 0.9\n",
    "\n",
    "GEMINI_CONFIG[\"project_config\"][\"qpm\"] = 100\n",
    "\n",
    "gemini_1 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_1['config_name'] = \"-1.0-pro-001\"\n",
    "\n",
    "gemini_2 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_2['config_name'] = \"-1.5-flash-001\"\n",
    "gemini_2['\"model\"'] = \"gemini-1.5-flash-001\"\n",
    "\n",
    "gemini_3 =  copy.deepcopy(GEMINI_CONFIG)\n",
    "gemini_3['config_name'] = \"-1.0-ultra-001\"\n",
    "gemini_3['\"model\"'] = \"gemini-1.0-ultra-001\"\n",
    "\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"gemini\": [\n",
    "        gemini_1,\n",
    "        gemini_2,\n",
    "        gemini_3\n",
    "         ],\n",
    "    \"palm\": [\n",
    "        palm_1, \n",
    "        palm_2\n",
    "        ],\n",
    "    # \"claude\": [\n",
    "    #     CLAUDE_CONFIG\n",
    "    # ]\n",
    "}\n",
    "\n",
    "ann = Annotate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating tasks: 100%|██████████| 250/250 [00:00<00:00, 52410.46it/s]\n",
      "Gathering gemini_-1.0-pro-001 results:   0%|          | 0/50 [00:00<?, ?it/s]2024-05-30 19:33:32,490/Annotate[ERROR]: gemini_-1.0-pro-001 Task 0 failed: Content has no parts.\n",
      "Gathering gemini_-1.0-pro-001 results: 100%|██████████| 50/50 [02:19<00:00,  2.79s/it]  \n",
      "Gathering gemini_-1.5-flash-001 results:   0%|          | 0/50 [00:00<?, ?it/s]2024-05-30 19:33:32,495/Annotate[ERROR]: gemini_-1.5-flash-001 Task 0 failed: Content has no parts.\n",
      "Gathering gemini_-1.5-flash-001 results: 100%|██████████| 50/50 [00:00<00:00, 34340.13it/s]\n",
      "Gathering gemini_-1.0-ultra-001 results:   0%|          | 0/50 [00:00<?, ?it/s]2024-05-30 19:33:32,499/Annotate[ERROR]: gemini_-1.0-ultra-001 Task 0 failed: Content has no parts.\n",
      "Gathering gemini_-1.0-ultra-001 results: 100%|██████████| 50/50 [00:00<00:00, 43772.74it/s]\n",
      "Gathering palm_temp_0.4 results: 100%|██████████| 50/50 [00:00<00:00, 143739.00it/s]\n",
      "Gathering palm_temp_0.9 results: 100%|██████████| 50/50 [00:00<00:00, 125577.96it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dict = await ann.classification(prompt, models, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = {}\n",
    "\n",
    "for k in output_dict.keys():\n",
    "    llm_response[k] = []\n",
    "    for idx, r in enumerate(output_dict[k]):\n",
    "        if r is not None:\n",
    "            stripped_r = r.strip().strip(\"'\")\n",
    "            if stripped_r in dataset['choices'][idx]:\n",
    "                llm_response[k].append(dataset['choices'][idx].index(stripped_r))\n",
    "            else:\n",
    "                # Handle case where stripped_r is not found in choices\n",
    "                llm_response[k].append(None)\n",
    "        else:\n",
    "            # Handle None values appropriately\n",
    "            llm_response[k].append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gemini_-1.0-pro-001': [None,\n",
       "  1,\n",
       "  None,\n",
       "  None,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  None,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  None,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  3,\n",
       "  None,\n",
       "  1,\n",
       "  2,\n",
       "  None,\n",
       "  None,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3],\n",
       " 'gemini_-1.5-flash-001': [None,\n",
       "  0,\n",
       "  None,\n",
       "  None,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  None,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  None,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  None,\n",
       "  None,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3],\n",
       " 'gemini_-1.0-ultra-001': [None,\n",
       "  0,\n",
       "  None,\n",
       "  None,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  None,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  None,\n",
       "  1,\n",
       "  None,\n",
       "  0,\n",
       "  3,\n",
       "  3],\n",
       " 'palm_temp_0.4': [2,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  None,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  None,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3],\n",
       " 'palm_temp_0.9': [2,\n",
       "  0,\n",
       "  None,\n",
       "  3,\n",
       "  None,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  None,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  None,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  None,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  None,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  3]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"./data/{now}/llm_response_{n_sample}__{now}.json\", \"r\") as f:\n",
    "#     llm_response= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nones(data):\n",
    "    df = pd.DataFrame.from_dict(llm_response).dropna().astype(int)\n",
    "    print(df.shape)\n",
    "    data = df.to_dict(orient=\"list\")\n",
    "\n",
    "    return data, df.index.to_list()\n",
    "\n",
    "def convert_dict_to_indexed_list(data_dict):\n",
    "    number_map = {key: index for index, key in enumerate(data_dict.keys())}\n",
    "    max_len = len(next(iter(data_dict.values())))\n",
    "\n",
    "    result = []\n",
    "    for index in range(max_len):\n",
    "        for key, value_list in data_dict.items():\n",
    "            value = value_list[index]\n",
    "            converted_value = value \n",
    "            result.append([index, number_map[key], converted_value])\n",
    "    return result\n",
    "    \n",
    "\n",
    "def generate_task_config(response_dict, num_classes):\n",
    "\n",
    "    num_labels = sum(len(lst) for lst in response_dict.values())\n",
    "    num_tasks =  len(list(response_dict.values())[0])\n",
    "    num_labelers = len(response_dict)\n",
    "    z  = 1/num_classes\n",
    "\n",
    "\n",
    "    tc = [num_labels, num_labelers, num_tasks, num_classes]\n",
    "    tc.extend([z] * tc[-1])\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 5)\n"
     ]
    }
   ],
   "source": [
    "n_class = 4\n",
    "llm_response, keep_idx = drop_nones(llm_response)\n",
    "task_conf = generate_task_config(llm_response, n_class)\n",
    "llm_result_list = convert_dict_to_indexed_list(llm_response)\n",
    "llm_result_list.insert(0, task_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 42,\n",
       " 43,\n",
       " 47,\n",
       " 48,\n",
       " 49]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dict = {\"gt\": dataset[\"answer\"]}\n",
    "gt_dict[\"gt\"] = [x for i, x in enumerate(gt_dict[\"gt\"]) if i in keep_idx]\n",
    "\n",
    "gt_dict[\"question\"] = [q for i,q in enumerate(dataset['question']) if i in keep_idx]\n",
    "\n",
    "len(gt_dict[\"gt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(f\"./data/{now}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(f\"./data/{now}/llm_response_{n_sample}__{now}.json\", \"w\") as json_file:\n",
    "    json.dump(llm_response, json_file)\n",
    "\n",
    "pd.DataFrame(gt_dict).to_csv(f\"./data/{now}/gt_{n_sample}__{now}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/20240530/llm_response_50__20240530.txt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"./data/{now}/llm_response_{n_sample}__{now}.txt\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    for sublist in llm_result_list:\n",
    "        line = \" \".join(str(num) for num in sublist)  # Convert to string, join with spaces\n",
    "        file.write(line + \"\\n\")  # Write line and add newline\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import glad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "glad_output = glad(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': {0: 0.7554699478616163,\n",
       "  1: 0.7381433730304823,\n",
       "  2: 0.8334161418318888,\n",
       "  3: 1.5139108516121806,\n",
       "  4: 1.162811617984595},\n",
       " 'beta': {0: 1.9719616389488468,\n",
       "  1: 0.9091598191195672,\n",
       "  2: 4.1487995059356315,\n",
       "  3: 4.1487995059356315,\n",
       "  4: 0.8703399675666524,\n",
       "  5: 1.3226648405224026,\n",
       "  6: 0.9091598191195672,\n",
       "  7: 4.1487995059356315,\n",
       "  8: 4.1487995059356315,\n",
       "  9: 1.881174058837864,\n",
       "  10: 4.1487995059356315,\n",
       "  11: 4.1487995059356315,\n",
       "  12: 4.1487995059356315,\n",
       "  13: 4.1487995059356315,\n",
       "  14: 4.1487995059356315,\n",
       "  15: 4.1487995059356315,\n",
       "  16: 1.2950626218383634,\n",
       "  17: 4.1487995059356315,\n",
       "  18: 1.2849976092135704,\n",
       "  19: 4.1487995059356315,\n",
       "  20: 4.1487995059356315,\n",
       "  21: 4.1487995059356315,\n",
       "  22: 4.1487995059356315,\n",
       "  23: 4.1487995059356315,\n",
       "  24: 4.1487995059356315,\n",
       "  25: 1.3226648405224026,\n",
       "  26: 1.0957418282351776,\n",
       "  27: 4.1487995059356315,\n",
       "  28: 4.1487995059356315,\n",
       "  29: 4.148799505935632,\n",
       "  30: 1.127923795081271,\n",
       "  31: 1.3226648405224026,\n",
       "  32: 4.148799505935632,\n",
       "  33: 4.1487995059356315,\n",
       "  34: 1.9936316846368114,\n",
       "  35: 4.1487995059356315,\n",
       "  36: 1.881174058837864,\n",
       "  37: 4.1487995059356315,\n",
       "  38: 4.1487995059356315},\n",
       " 'probZ': {0: {0: 0.9999565353277847,\n",
       "   1: 3.778600810369459e-05,\n",
       "   2: 2.83933205577892e-06,\n",
       "   3: 2.83933205577892e-06},\n",
       "  1: {0: 0.0030436037386109413,\n",
       "   1: 0.6816530507036695,\n",
       "   2: 0.31225974181910854,\n",
       "   3: 0.0030436037386109413},\n",
       "  2: {0: 3.968531503627578e-12,\n",
       "   1: 0.9999999999880943,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 3.968531503627578e-12},\n",
       "  3: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 0.9999999999880944,\n",
       "   3: 3.968531503627578e-12},\n",
       "  4: {0: 0.0422310953754913,\n",
       "   1: 0.007404654869911166,\n",
       "   2: 0.6847076532818699,\n",
       "   3: 0.26565659647272755},\n",
       "  5: {0: 0.976693619786033,\n",
       "   1: 0.0003484011327601248,\n",
       "   2: 0.022609577948446574,\n",
       "   3: 0.0003484011327601248},\n",
       "  6: {0: 0.0030436037386109413,\n",
       "   1: 0.6816530507036695,\n",
       "   2: 0.31225974181910854,\n",
       "   3: 0.0030436037386109413},\n",
       "  7: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 0.9999999999880944,\n",
       "   3: 3.968531503627578e-12},\n",
       "  8: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 0.9999999999880944,\n",
       "   3: 3.968531503627578e-12},\n",
       "  9: {0: 0.9999207648241758,\n",
       "   1: 6.956543178233012e-05,\n",
       "   2: 4.8348720209878325e-06,\n",
       "   3: 4.8348720209878325e-06},\n",
       "  10: {0: 3.968531503627578e-12,\n",
       "   1: 0.9999999999880943,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 3.968531503627578e-12},\n",
       "  11: {0: 3.968531503627578e-12,\n",
       "   1: 0.9999999999880943,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 3.968531503627578e-12},\n",
       "  12: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 0.9999999999880944,\n",
       "   3: 3.968531503627578e-12},\n",
       "  13: {0: 3.968531503627578e-12,\n",
       "   1: 0.9999999999880943,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 3.968531503627578e-12},\n",
       "  14: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 0.9999999999880944},\n",
       "  15: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 0.9999999999880944,\n",
       "   3: 3.968531503627578e-12},\n",
       "  16: {0: 0.0004314422003700179,\n",
       "   1: 0.9923930503346949,\n",
       "   2: 0.003808809429028537,\n",
       "   3: 0.0033666980359065867},\n",
       "  17: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 0.9999999999880944,\n",
       "   3: 3.968531503627578e-12},\n",
       "  18: {0: 0.003995920009160292,\n",
       "   1: 0.0004564499627661243,\n",
       "   2: 0.9919325529758827,\n",
       "   3: 0.003615077052190875},\n",
       "  19: {0: 3.968531503627578e-12,\n",
       "   1: 0.9999999999880943,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 3.968531503627578e-12},\n",
       "  20: {0: 3.968531503627578e-12,\n",
       "   1: 0.9999999999880943,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 3.968531503627578e-12},\n",
       "  21: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 0.9999999999880944,\n",
       "   3: 3.968531503627578e-12},\n",
       "  22: {0: 3.968531503627578e-12,\n",
       "   1: 0.9999999999880943,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 3.968531503627578e-12},\n",
       "  23: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 0.9999999999880944},\n",
       "  24: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 0.9999999999880944,\n",
       "   3: 3.968531503627578e-12},\n",
       "  25: {0: 0.976693619786033,\n",
       "   1: 0.022609577948446574,\n",
       "   2: 0.0003484011327601248,\n",
       "   3: 0.0003484011327601248},\n",
       "  26: {0: 0.0011502893476943916,\n",
       "   1: 0.9129940136191634,\n",
       "   2: 0.08470540768544778,\n",
       "   3: 0.0011502893476943916},\n",
       "  27: {0: 3.968531503627578e-12,\n",
       "   1: 0.9999999999880943,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 3.968531503627578e-12},\n",
       "  28: {0: 3.968531503627578e-12,\n",
       "   1: 0.9999999999880943,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 3.968531503627578e-12},\n",
       "  29: {0: 0.9999999999880943,\n",
       "   1: 3.968531503627549e-12,\n",
       "   2: 3.968531503627549e-12,\n",
       "   3: 3.968531503627549e-12},\n",
       "  30: {0: 0.0010954112940519023,\n",
       "   1: 0.9791504139809538,\n",
       "   2: 0.012198403536839533,\n",
       "   3: 0.007555771188154725},\n",
       "  31: {0: 0.0003484011327601248,\n",
       "   1: 0.976693619786033,\n",
       "   2: 0.022609577948446574,\n",
       "   3: 0.0003484011327601248},\n",
       "  32: {0: 0.9999999999880943,\n",
       "   1: 3.968531503627549e-12,\n",
       "   2: 3.968531503627549e-12,\n",
       "   3: 3.968531503627549e-12},\n",
       "  33: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 0.9999999999880944},\n",
       "  34: {0: 3.2693282634578834e-05,\n",
       "   1: 0.9999623033132274,\n",
       "   2: 2.5017020690481618e-06,\n",
       "   3: 2.5017020690481618e-06},\n",
       "  35: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 0.9999999999880944,\n",
       "   3: 3.968531503627578e-12},\n",
       "  36: {0: 6.956543178233012e-05,\n",
       "   1: 0.9999207648241758,\n",
       "   2: 4.8348720209878325e-06,\n",
       "   3: 4.8348720209878325e-06},\n",
       "  37: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 0.9999999999880944},\n",
       "  38: {0: 3.968531503627578e-12,\n",
       "   1: 3.968531503627578e-12,\n",
       "   2: 3.968531503627578e-12,\n",
       "   3: 0.9999999999880944}},\n",
       " 'labels': {0: 0,\n",
       "  1: 1,\n",
       "  2: 1,\n",
       "  3: 2,\n",
       "  4: 2,\n",
       "  5: 0,\n",
       "  6: 1,\n",
       "  7: 2,\n",
       "  8: 2,\n",
       "  9: 0,\n",
       "  10: 1,\n",
       "  11: 1,\n",
       "  12: 2,\n",
       "  13: 1,\n",
       "  14: 3,\n",
       "  15: 2,\n",
       "  16: 1,\n",
       "  17: 2,\n",
       "  18: 2,\n",
       "  19: 1,\n",
       "  20: 1,\n",
       "  21: 2,\n",
       "  22: 1,\n",
       "  23: 3,\n",
       "  24: 2,\n",
       "  25: 0,\n",
       "  26: 1,\n",
       "  27: 1,\n",
       "  28: 1,\n",
       "  29: 0,\n",
       "  30: 1,\n",
       "  31: 1,\n",
       "  32: 0,\n",
       "  33: 3,\n",
       "  34: 1,\n",
       "  35: 2,\n",
       "  36: 1,\n",
       "  37: 3,\n",
       "  38: 3}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glad_output = glad(filename)\n",
    "glad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_vote(label_dict: Dict[str, List[int]]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Finds the majority value for each element across multiple lists within a dictionary.\n",
    "\n",
    "    Args:\n",
    "        label_dict: A dictionary where keys are identifiers and values are lists of labels.\n",
    "\n",
    "    Returns:\n",
    "        A list of majority values corresponding to each element position.\n",
    "    \"\"\"\n",
    "    list_of_labels = list(label_dict.values())  # Extract values into a list\n",
    "    majority_values = []\n",
    "\n",
    "    for elements in zip(*list_of_labels):\n",
    "        element_counts = Counter(elements)\n",
    "        most_common_element = element_counts.most_common(1)[0]\n",
    "        majority_values.append(most_common_element[0])\n",
    "\n",
    "    return majority_values\n",
    "\n",
    "\n",
    "def accuracy_with_none_penalty(y_true, y_pred):\n",
    "    filtered_y_true = []\n",
    "    filtered_y_pred = []\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if pred is not None:  # Only include non-None predictions\n",
    "            filtered_y_true.append(true)\n",
    "            filtered_y_pred.append(pred)\n",
    "        else:\n",
    "            filtered_y_true.append(true)  # Include true label\n",
    "            filtered_y_pred.append(-1)   # Replace None with wrong label (e.g., -1)\n",
    "\n",
    "    return accuracy_score(filtered_y_true, filtered_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/raw_llm_response_100__20240529.json\", \"r\") as f:\n",
    "#     llm_response = json.load(f)\n",
    "\n",
    "# df_glad = pd.read_csv(\"./data/label_glad__20240529.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response[\"majority\"] = get_majority_vote(llm_response)\n",
    "# llm_response[\"glad\"] = df_glad[\"label\"].values\n",
    "llm_response[\"glad\"] = list(glad_output['labels'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for k, v in llm_response.items():\n",
    "    d[k] = accuracy_with_none_penalty(dataset['answer'], v) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([d]).T.reset_index()\n",
    "df.columns = [\"model\", \"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glad_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard questions\n",
    "q_list = [q for i,q in enumerate(dataset['question']) if i in keep_idx]\n",
    "glad_output[\"beta\"]\n",
    "question_d = {\"question\": q_list,\n",
    "              \"task difficulty\": list(glad_output[\"beta\"].values()),\n",
    "              \"label confidence\": list(glad_output[\"probZ\"].values())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(question_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
